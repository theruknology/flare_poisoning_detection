# FLARE Codebase Analysis - Executive Summary

## Project At a Glance

**Name:** FLARE - Defending Federated Learning against Model Poisoning Attacks via Latent Space Representations  
**Paper:** AsiaCCS 2022  
**Repository:** flare_poisoning_detection  
**Language:** Python 3.6+  
**Framework:** TensorFlow 1.x + Keras  
**Purpose:** Research framework for federated learning (FL) with model poisoning attacks and Byzantine-robust defenses

---

## What This Project Does

### Core Functionality
- **Simulates federated learning:** Multiple agents train on distributed data, aggregate models periodically
- **Implements attacks:** Malicious agents inject backdoors/trojans to corrupt global model
- **Provides defenses:** 
  - Byzantine-robust aggregation rules (Krum, median, trimmed mean, etc.)
  - Anomaly detection using latent space analysis
  - Trust-based weighted aggregation

### Key Contributions (from Paper)
1. **Attack methods** optimized for specific aggregation rules
2. **Detection mechanism** using penultimate layer representations and MMD statistics
3. **Comprehensive evaluation** across multiple datasets and aggregation schemes

---

## Project Structure

```
flare_poisoning_detection/
‚îú‚îÄ‚îÄ CORE ORCHESTRATION
‚îÇ   ‚îî‚îÄ‚îÄ dist_train_w_attack.py ........................ Main training loop (entry point)
‚îÇ
‚îú‚îÄ‚îÄ GLOBAL CONFIGURATION
‚îÇ   ‚îî‚îÄ‚îÄ global_vars.py ........................ Command-line args, paths, dataset constants
‚îÇ
‚îú‚îÄ‚îÄ AGENT TRAINING
‚îÇ   ‚îú‚îÄ‚îÄ agents.py ........................ Benign agent local training
‚îÇ   ‚îî‚îÄ‚îÄ malicious_agent.py ........................ Malicious agent attack implementations
‚îÇ
‚îú‚îÄ‚îÄ AGGREGATION ALGORITHMS
‚îÇ   ‚îî‚îÄ‚îÄ agg_alg.py ........................ avg, krum, median, trimmed mean, bulyan
‚îÇ
‚îú‚îÄ‚îÄ DEFENSE MECHANISMS
‚îÇ   ‚îî‚îÄ‚îÄ detect.py ........................ Anomaly detection using MMD + trust scoring
‚îÇ
‚îú‚îÄ‚îÄ ATTACKS
‚îÇ   ‚îî‚îÄ‚îÄ attack.py ........................ GAR-specific attack optimization
‚îÇ
‚îú‚îÄ‚îÄ UTILITIES
‚îÇ   ‚îú‚îÄ‚îÄ io_utils.py ........................ Dataset loading, malicious data generation
‚îÇ   ‚îú‚îÄ‚îÄ eval_utils.py ........................ Model evaluation, backdoor testing
‚îÇ   ‚îú‚îÄ‚îÄ mnist.py, cifar_utils.py, census_utils.py, kather_utils.py
‚îÇ   ‚îú‚îÄ‚îÄ dist_utils.py ........................ Weight distance calculations
‚îÇ   ‚îú‚îÄ‚îÄ mmd.py ........................ Statistical Maximum Mean Discrepancy
‚îÇ   ‚îî‚îÄ‚îÄ [other utilities]
‚îÇ
‚îú‚îÄ‚îÄ DEMONSTRATIONS
‚îÇ   ‚îî‚îÄ‚îÄ yolo_demo.py ........................ YOLO integration example
‚îÇ
‚îú‚îÄ‚îÄ EXPERIMENT CONFIGS
‚îÇ   ‚îî‚îÄ‚îÄ script/bash_{dataset}/ ........................ Pre-configured bash scripts
‚îÇ
‚îî‚îÄ‚îÄ DOCUMENTATION
    ‚îú‚îÄ‚îÄ README.md ........................ Original readme
    ‚îú‚îÄ‚îÄ CODEBASE_ANALYSIS.md ........................ This analysis (detailed)
    ‚îú‚îÄ‚îÄ ARCHITECTURE.md ........................ Architecture diagrams & flows
    ‚îî‚îÄ‚îÄ QUICKSTART.md ........................ Quick start guide
```

---

## How to Run It

### Fastest Way to Get Started
```bash
cd /home/theruknology/Desktop/reswork/flare_poisoning_detection

# Install dependencies
pip install tensorflow keras numpy scipy scikit-learn matplotlib

# Run quick test
python dist_train_w_attack.py --dataset MNIST --k 10 --E 2 --T 5 --train
```

### Standard Experiment
```bash
# Train benign FL baseline
python dist_train_w_attack.py \
  --dataset MNIST --k 50 --C 0.2 --E 5 --T 30 --train \
  --gar avg --detect_method none

# Train with attack
python dist_train_w_attack.py \
  --dataset MNIST --k 50 --C 0.2 --E 5 --T 30 --train \
  --mal --mal_obj target_backdoor --attack_type backdoor --gar avg

# Train with defense (robust aggregation)
python dist_train_w_attack.py \
  --dataset MNIST --k 50 --C 0.2 --E 5 --T 30 --train \
  --mal --attack_type backdoor --gar krum

# Train with defense (detection)
python dist_train_w_attack.py \
  --dataset MNIST --k 50 --C 0.2 --E 5 --T 30 --train \
  --mal --attack_type backdoor --detect --detect_method detect_penul
```

---

## Key Concepts

### Federated Learning
- **Idea:** Train global model by aggregating local updates from multiple agents
- **Key parameter:** `--k` (number of agents), `--C` (fraction per round), `--E` (local epochs)
- **Process:** Each round: select subset ‚Üí train locally ‚Üí aggregate weights ‚Üí repeat

### Model Poisoning Attack
- **Goal:** Inject malicious updates to corrupt global model
- **Two types:**
  1. **Backdoor:** Misclassify specific "trojan" inputs to target label
  2. **Untargeted:** Degrade overall model accuracy

### Byzantine Aggregation
- **Problem:** Simple averaging vulnerable to poisoned updates
- **Solution:** Robust rules that ignore outliers:
  - **Krum:** Select agent with minimum sum of distances
  - **Median:** Coordinate-wise median
  - **Trimmed Mean:** Remove extremes before averaging
  - **Bulyan:** Multi-layer robust aggregation

### Latent Space Detection (FLARE)
- **Key insight:** Malicious models have different "representations" in penultimate layer
- **Method:** 
  1. Extract penultimate layer features on test data
  2. Compute distribution distances (MMD) between agents
  3. Generate trust scores (high=benign, low=malicious)
  4. Weight aggregation by trust scores

---

## Critical Files

| File | Lines | Purpose |
|------|-------|---------|
| `dist_train_w_attack.py` | 301 | Main orchestrator, training loop |
| `global_vars.py` | 200+ | Argument parsing, configuration |
| `agents.py` | 300+ | Benign agent training logic |
| `malicious_agent.py` | 861 | Attack implementations |
| `agg_alg.py` | 400+ | Aggregation algorithms |
| `detect.py` | 426 | Detection mechanisms |
| `attack.py` | 300+ | Attack optimizations |
| `eval_utils.py` | 280 | Model evaluation utilities |
| `io_utils.py` | 201 | Dataset I/O, data generation |

**Total Code:** ~3,500+ lines of Python

---

## Key Parameters Explained

| Parameter | Purpose | Default | Range |
|-----------|---------|---------|-------|
| `--dataset` | Dataset type | kather | MNIST, fMNIST, CIFAR-10, census, kather |
| `--k` | # agents | 10 | 5-100 |
| `--C` | Fraction per round | 1.0 | 0.1-1.0 |
| `--E` | Local epochs | 5 | 1-20 |
| `--T` | Total rounds | 40 | 5-100 |
| `--B` | Batch size | 50 | 16-128 |
| `--eta` | Learning rate | 1e-3 | 1e-5 to 1e-1 |
| `--mal` | Enable attacks | False | True/False |
| `--gar` | Aggregation rule | krum | avg, krum, coomed, trimmedmean, bulyan |
| `--detect` | Enable detection | False | True/False |
| `--train` | Training mode | False | True/False |

---

## What Gets Generated

### After Training
```
weights/{dataset}/model_{num}/{optimizer}/k{k}_E{E}_B{B}_C{C}_lr{eta}/
  ‚îú‚îÄ‚îÄ global_weights_t0.npy
  ‚îú‚îÄ‚îÄ global_weights_t1.npy
  ‚îÇ ... (one per round)
  ‚îî‚îÄ‚îÄ ben_delta_{i}_t{t}.npy (per agent per round)

output_files/{dataset}/.../*.txt
  ‚îî‚îÄ‚îÄ Training logs with metrics

result/{dataset}/
  ‚îú‚îÄ‚îÄ acc_*.csv (accuracy per round)
  ‚îî‚îÄ‚îÄ mmd_*.csv (anomaly scores if detection)
```

### Example Results
- **Benign baseline:** Accuracy ‚Üí 99% (MNIST)
- **With attack (avg):** Backdoor success ‚Üí 100%, accuracy ‚Üí 98%
- **With attack (Krum):** Backdoor success ‚Üí 10%, accuracy ‚Üí 99%
- **With attack (detection):** Backdoor success ‚Üí 5%, accuracy ‚Üí 99%

---

## Attack Types Supported

| Attack | GAR Target | Complexity | Effectiveness |
|--------|-----------|-----------|---|
| `backdoor_krum` | Krum | High | Optimized for Krum |
| `backdoor_coomed` | Coordinate median | High | Optimized for median |
| `backdoor` | Average | Medium | Generic backdoor |
| `untargeted_krum` | Krum | Medium | Prevents convergence |
| `data_poison` | Any | Low | Data-level poisoning |

---

## Defense Methods Compared

| Defense | Method | Complexity | Robustness |
|---------|--------|-----------|-----------|
| **Average (baseline)** | Simple mean | O(k) | Low |
| **Krum** | Outlier detection | O(k¬≤) | Medium |
| **Median** | Coordinate-wise | O(k log k) | Medium-High |
| **Trimmed Mean** | Percentile removal | O(k log k) | Medium-High |
| **Bulyan** | Multi-layer robust | O(Œ∏¬∑k) | High |
| **Detection (FLARE)** | Trust scoring | O(k¬≤) | High |
| **Detection + Krum** | Combination | O(k¬≤) | Very High |

---

## Dependencies

### Core Requirements
```
tensorflow>=1.15 (with compat.v1)
keras
numpy
scipy
scikit-learn
matplotlib
```

### Optional
- CUDA 10.0+ for GPU acceleration
- cuDNN for GPU acceleration

### No requirements.txt Found
- Install manually: `pip install tensorflow keras numpy scipy scikit-learn matplotlib`

---

## Typical Experiment Timeline

### Small Test (MNIST, 10 agents, 5 rounds)
- **CPU:** ~2 minutes
- **GPU:** ~30 seconds
- **Memory:** 1GB

### Medium (MNIST, 50 agents, 30 rounds)
- **CPU:** Not recommended
- **GPU (1):** 1-2 hours
- **GPU (4):** 30 min - 1 hour
- **Memory:** 4-8GB

### Large (Kather, 10 agents, 30 rounds)
- **GPU:** 2-3 hours
- **Memory:** 8GB

---

## Common Modifications for Research

### Add New Attack
1. Implement in `malicious_agent.py::mal_agent()`
2. Register in `agg_alg.py` or `attack.py`
3. Add `--attack_type` choice in `global_vars.py`

### Add New Aggregation Rule
1. Create function in `agg_alg.py`
2. Add case in `dist_train_w_attack.py::train_fn()`
3. Register in `--gar` choices in `global_vars.py`

### Add New Detection Method
1. Add method to `Detect` class in `detect.py`
2. Call in `dist_train_w_attack.py::train_fn()`
3. Register in `--detect_method` choices

---

## Key Insights from the Code

1. **Multiprocessing:** Uses Python `multiprocessing.Manager` for inter-process communication
   - Each agent trains in separate process
   - Updates shared via manager dictionary

2. **TensorFlow 1.x:** Uses compatibility mode (`tf.compat.v1`) for newer versions
   - Manual session management
   - Placeholders for input

3. **Modular Design:** Clean separation:
   - Agents (training)
   - Aggregation (combining updates)
   - Detection (anomaly scoring)
   - Attacks (exploitation)
   - Evaluation (testing)

4. **Scalability:** 
   - Supports multiple GPUs
   - Distributes agents across GPUs
   - Memory fraction control

5. **Reproducibility:**
   - Seed control for RNG
   - Saved weights per round
   - CSV result logging

---

## Performance Characteristics

### Complexity Analysis
- **Per round:** O(k*E*|shard|/B) for training + O(GAR) for aggregation
- **Per agent:** O(E*|shard|/B) gradient updates
- **Aggregation (avg):** O(k*d) where d=model dimension
- **Aggregation (Krum):** O(k¬≤*d)
- **Detection:** O(k¬≤*n*d) for n test samples, d features

### Scalability Limits
- **Agents:** 50-100 (per-GPU limits)
- **Model size:** 1-100 million parameters (dataset dependent)
- **Test set:** Usually 10k samples
- **GPU memory:** 4-16 GB per GPU

---

## References & Citations

**Paper:**
```
FLARE: Defending Federated Learning against Model Poisoning Attacks 
via Latent Space Representations
17th ACM ASIA Conference on Computer and Communications Security (AsiaCCS 2022)
```

**Related Work:**
- Byzantine-robust aggregation: Krum, Median, Trimmed Mean
- Federated Learning: FedAvg (McMahan et al.)
- Model Poisoning: Label-flipping, Backdoor attacks
- Detection: Anomaly detection via latent space analysis

---

## What's NOT in This Project

- Differential privacy (privacy-preserving learning)
- Gradient compression (communication efficiency)
- Asynchronous aggregation (straggler handling)
- Personalized FL (client-specific models)
- Cross-device FL (mobile/edge devices)
- Continual learning (evolving tasks)

---

## Final Notes

### Strengths
‚úÖ Comprehensive framework for FL research  
‚úÖ Multiple attack and defense mechanisms  
‚úÖ Clean, modular code structure  
‚úÖ Supports multiple datasets and models  
‚úÖ Pre-configured scripts for quick experiments  

### Limitations
‚ö†Ô∏è TensorFlow 1.x (older framework version)  
‚ö†Ô∏è Single-machine simulation (not true distributed)  
‚ö†Ô∏è No requirements.txt provided  
‚ö†Ô∏è Limited documentation in code  
‚ö†Ô∏è No unit tests  

### Best For
- üéì Academic research in FL security
- üìö Learning about poisoning attacks & defenses
- üî¨ Experimenting with aggregation rules
- üìä Benchmarking attack/defense combinations

---

## Documentation Files Created

1. **CODEBASE_ANALYSIS.md** - Comprehensive file-by-file analysis (detailed reference)
2. **ARCHITECTURE.md** - System architecture, diagrams, flows, parameter combinations
3. **QUICKSTART.md** - Practical quick-start guide with example workflows
4. **This file** - Executive summary & quick reference

---

**Last Updated:** November 11, 2025  
**Analysis Scope:** All Python files, configuration scripts, documentation  
**Total Code Lines:** ~3,500+
